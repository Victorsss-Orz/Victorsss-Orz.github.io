---
title: "Language Models are Few-Shot Graders"
collection: publications
category: conferences
permalink: /publications/2025-07-few-shot-grading
date: 2025-07-20
excerpt: ""
venue: "26th International Conference on Artificial Intelligence in Education"
paperurl: "https://doi.org/10.1007/978-3-031-98459-4_1"
citation: "Zhao, C., Silva, M., Poulsen, S. (2025). Language Models are Few-Shot Graders. In: Cristea, A.I., Walker, E., Lu, Y., Santos, O.C., Isotani, S. (eds) Artificial Intelligence in Education. AIED 2025. Lecture Notes in Computer Science, vol 15880. Springer, Cham. https://doi.org/10.1007/978-3-031-98459-4_1"
---

**Abstract:** Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.
